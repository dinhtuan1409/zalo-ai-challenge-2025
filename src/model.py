import os
import math
import torch
import random
import numpy as np
import torch.nn as nn
import json # M·ªöI: ƒê·ªÉ l∆∞u metadata
from torch.utils.data import DataLoader, Subset
from tqdm import tqdm
from torch.cuda.amp import autocast, GradScaler
from transformers import AutoTokenizer # M·ªöI: C·∫ßn cho LLM

# --- M·ªöI: Gi·∫£ s·ª≠ b·∫°n ƒë√£ c·∫≠p nh·∫≠t c√°c file n√†y ---
# (B·∫°n c·∫ßn t·ª± t·∫°o c√°c file n√†y d·ª±a tr√™n h∆∞·ªõng d·∫´n tr∆∞·ªõc)
from model import VideoTextLLMQA_V2 # THAY ƒê·ªîI: Import m√¥ h√¨nh V2
from dataset import (
    FeatureVideoQADatasetMPNET, # Gi·∫£ s·ª≠ dataset n√†y ƒë√£ ƒë∆∞·ª£c s·ª≠a
    collate_fn_mpnet            # Gi·∫£ s·ª≠ collate_fn n√†y ƒë√£ ƒë∆∞·ª£c s·ª≠a
)

# ===========================
# CONFIG
# ===========================
DATA_JSON = "/kaggle/input/zalo-ai-challenge-2025-roadbuddy/traffic_buddy_train+public_test/train/train.json"
VIDEO_FEAT_DIR = "Feature/train"

# --- Config cho m√¥ h√¨nh V2 ---
LLM_MODEL_NAME = "mistralai/Mistral-7B-v0.1" # ƒê√É S·ª¨A: D√πng Mistral 7B (Open Access)
VIDEO_FEAT_DIM = 2304 
TEXT_FEAT_DIM = 768 # Gi·ªØ nguy√™n dim c·ªßa text_feats (MPNet/CLIP)

# --- Config hu·∫•n luy·ªán (ƒêi·ªÅu ch·ªânh cho PEFT) ---
BATCH_SIZE = 4        # THAY ƒê·ªîI: Gi·∫£m BS v√¨ LLM t·ªën VRAM
ACCUM_STEPS = 4       # THAY ƒê·ªîI: TƒÉng ACCUM (Effective BS = 4*4 = 16)
LR = 1e-4             # THAY ƒê·ªîI: Learning rate ph·ªï bi·∫øn cho LoRA
EPOCHS = 10           # Gi·∫£m epochs, v√¨ LLM h·ªôi t·ª• nhanh h∆°n
WEIGHT_DECAY = 0.01
VALID_SPLIT = 0.1
OUTPUT_DIR = "/kaggle/working/"

SEED = 42
USE_FP16 = True
EARLYSTOP_PATIENCE = 2 # Gi·∫£m patience
CLIP_NORM = 1.0
NUM_WORKERS = os.cpu_count()

# ===========================
# SEED
# ===========================
def seed_everything(seed=SEED):
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)

# ===========================
# EVALUATE
# ===========================
def evaluate(model, loader, loss_fn, device):
    model.eval()
    total_loss = 0.0
    total_correct = 0
    total_samples = 0

    with torch.no_grad():
        pbar = tqdm(loader, desc="Eval", leave=False)
        for batch in pbar:
            # THAY ƒê·ªîI: Unpack batch cho m√¥ h√¨nh V2
            video_feats = batch["video_feats"].to(device)
            text_feats = batch["text_feats"].to(device) # V·∫´n c·∫ßn text_feats
            labels = batch["labels"].to(device)
            questions = batch["questions"]       # M·ªöI: list[str]
            choice_texts = batch["choice_texts"] # M·ªöI: list[list[str]]

            with autocast(enabled=USE_FP16):
                # THAY ƒê·ªîI: Truy·ªÅn input m·ªõi cho model
                logits = model(video_feats, text_feats, questions, choice_texts)
                loss = loss_fn(logits, labels)

            total_loss += loss.item() * video_feats.size(0)
            
            mask = labels != -1 
            if mask.sum() > 0:
                preds = logits.argmax(dim=1)
                total_correct += (preds[mask] == labels[mask]).sum().item()
                total_samples += mask.sum().item()

    avg_loss = total_loss / len(loader.dataset)
    avg_acc = total_correct / total_samples if total_samples > 0 else 0.0
    return avg_loss, avg_acc

# ===========================
# MAIN TRAIN LOOP
# ===========================
def train_loop():
    seed_everything()
    device = "cuda" if torch.cuda.is_available() else "cpu"

    # --------------------------
    # M·ªöI: Load Tokenizer (thay v√¨ text_encoder)
    # --------------------------
    print("Loading tokenizer...")
    tokenizer = AutoTokenizer.from_pretrained(LLM_MODEL_NAME)
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token
    
    print("Loading dataset...")
    full_ds = FeatureVideoQADatasetMPNET( # GI·∫¢ S·ª¨ file n√†y ƒë√£ ƒë∆∞·ª£c s·ª≠a
        json_path=DATA_JSON,
        video_feat_dir=VIDEO_FEAT_DIR,
        video_feat_dim=VIDEO_FEAT_DIM,
        # text_encoder b·ªã x√≥a
        tokenizer=tokenizer, # M·ªöI: Truy·ªÅn tokenizer v√†o dataset
        preload_text=True, # Gi·∫£ s·ª≠ b·∫°n v·∫´n preload text_feats
        is_test=False
    )

    n = len(full_ds)
    n_val = max(1, int(n * VALID_SPLIT))
    indices = list(range(n))
    random.shuffle(indices)
    val_idx, train_idx = indices[:n_val], indices[n_val:]

    train_ds = Subset(full_ds, train_idx)
    val_ds = Subset(full_ds, val_idx)

    train_loader = DataLoader(
        train_ds,
        batch_size=BATCH_SIZE, # ƒê√£ c·∫≠p nh·∫≠t
        shuffle=True,
        collate_fn=collate_fn_mpnet, # GI·∫¢ S·ª¨ file n√†y ƒë√£ ƒë∆∞·ª£c s·ª≠a
        num_workers=NUM_WORKERS,
        pin_memory=True 
    )
    val_loader = DataLoader(
        val_ds,
        batch_size=BATCH_SIZE * 2,
        shuffle=False,
        collate_fn=collate_fn_mpnet, # GI·∫¢ S·ª¨ file n√†y ƒë√£ ƒë∆∞·ª£c s·ª≠a
        num_workers=NUM_WORKERS,
        pin_memory=True
    )

    # --------------------------
    # Build model (THAY ƒê·ªîI)
    # --------------------------
    print("Building model (V2)...")
    model = VideoTextLLMQA_V2(
        video_dim=VIDEO_FEAT_DIM,
        text_dim=TEXT_FEAT_DIM, # Dim c·ªßa text_feats (MPNet/CLIP)
        hidden_dim=512,
        llm_model_name=LLM_MODEL_NAME, # ƒê√£ l√† Mistral
        device=device
        # Model V2 t·ª± x·ª≠ l√Ω device_map v√† PEFT b√™n trong
    )

    # T·∫°m th·ªùi t·∫Øt torch.compile, n√≥ c√≥ th·ªÉ kh√¥ng t∆∞∆°ng th√≠ch t·ªët v·ªõi PEFT/HF
    # if hasattr(torch, 'compile'):
    #     print("Compiling model (PyTorch 2.0+)...")
    #     model = torch.compile(model)

    # --------------------------
    # Optimizer (THAY ƒê·ªîI L·ªöN)
    # --------------------------
    print("Setting up PEFT optimizer...")
    # M·ªöI: Ch·ªâ l·∫•y c√°c tham s·ªë c√≥ th·ªÉ hu·∫•n luy·ªán (LoRA, projections, v.v.)
    trainable_params = []
    for name, param in model.named_parameters():
        if param.requires_grad:
            trainable_params.append(param)
            print(f"Adding trainable param: {name}")

    optimizer = torch.optim.AdamW(
        trainable_params, # THAY ƒê·ªîI: Ch·ªâ truy·ªÅn c√°c params n√†y
        lr=LR,
        weight_decay=WEIGHT_DECAY
    )

    loss_fn = nn.CrossEntropyLoss(ignore_index=-1)
    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(
        optimizer, mode='min', factor=0.5, patience=1, verbose=True
    )
    scaler = GradScaler(enabled=USE_FP16)

    # --------------------------
    # Training loop
    # --------------------------
    best_val_loss = float('inf')
    epochs_no_improve = 0
    os.makedirs(OUTPUT_DIR, exist_ok=True)

    print(f"--- Starting PEFT training ---")
    print(f"Device: {device}, FP16: {USE_FP16}, Effective BS: {BATCH_SIZE * ACCUM_STEPS}")
    print(f"Train samples: {len(train_ds)}, Val samples: {len(val_ds)}")
    
    for epoch in range(EPOCHS):
        model.train()
        pbar = tqdm(train_loader, desc=f"Epoch {epoch+1}/{EPOCHS} train")
        total_loss = 0.0
        
        optimizer.zero_grad() 

        for step, batch in enumerate(pbar):
            # THAY ƒê·ªîI: Unpack batch cho m√¥ h√¨nh V2
            video_feats = batch["video_feats"].to(device)
            text_feats = batch["text_feats"].to(device)
            labels = batch["labels"].to(device)
            questions = batch["questions"]       # M·ªöI
            choice_texts = batch["choice_texts"] # M·ªöI

            with autocast(enabled=USE_FP16):
                # THAY ƒê·ªîI: Truy·ªÅn input m·ªõi cho model
                logits = model(video_feats, text_feats, questions, choice_texts)
                loss = loss_fn(logits, labels)
                loss_to_backward = loss / ACCUM_STEPS 

            scaler.scale(loss_to_backward).backward()

            if (step + 1) % ACCUM_STEPS == 0:
                scaler.unscale_(optimizer)
                # THAY ƒê·ªîI: Clip grad norm ch·ªâ cho c√°c tham s·ªë hu·∫•n luy·ªán
                torch.nn.utils.clip_grad_norm_(trainable_params, CLIP_NORM)
                scaler.step(optimizer)
                scaler.update()
                optimizer.zero_grad()

            total_loss += loss.item()
            avg_loss = total_loss / (step + 1)
            pbar.set_postfix({"loss": f"{avg_loss:.4f}"})

        # --------------------------
        # Evaluate
        # --------------------------
        val_loss, val_acc = evaluate(model, val_loader, loss_fn, device)
        print(f"‚úÖ Epoch {epoch+1} - Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}")
        scheduler.step(val_loss)

        # --------------------------
        # Save best (THAY ƒê·ªîI L·ªöN)
        # --------------------------
        if val_loss < best_val_loss:
            best_val_loss = val_loss
            epochs_no_improve = 0
            
            # THAY ƒê·ªîI: L∆∞u adapter (PEFT)
            adapter_path = os.path.join(OUTPUT_DIR, "best_adapter")
            model.save_pretrained(adapter_path) # ƒê√¢y l√† c√°ch l∆∞u c·ªßa PEFT
            
            # L∆∞u c√°c th√¥ng tin kh√°c
            meta_path = os.path.join(OUTPUT_DIR, "best_model_meta.json")
            with open(meta_path, 'w') as f:
                json.dump({
                    "val_loss": val_loss,
                    "val_acc": val_acc,
                    "epoch": epoch
                }, f)
            
            print(f"üíæ Saved best adapter to {adapter_path}!")
        else:
            epochs_no_improve += 1
            print(f"No improvement for {epochs_no_improve} epoch(s)")
            if epochs_no_improve >= EARLYSTOP_PATIENCE:
                print("‚õî Early stopping triggered!")
                break

    print(f"‚úÖ Training done. Best val_loss: {best_val_loss:.4f}")

# ===========================
# RUN
# ===========================
if __name__ == "__main__":
    train_loop()